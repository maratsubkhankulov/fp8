{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to py39 (Python 3.9.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the current file's directory to the Python path\n",
    "current_dir = os.path.dirname(os.path.abspath('..'))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "\n",
    "from solution import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP8 E4M3 range:\n",
      "tensor([ 0.0000e+00,  1.9531e-03,  3.9062e-03,  5.8594e-03,  7.8125e-03,\n",
      "         9.7656e-03,  1.1719e-02,  1.3672e-02,  1.5625e-02,  1.7578e-02,\n",
      "         1.9531e-02,  2.1484e-02,  2.3438e-02,  2.5391e-02,  2.7344e-02,\n",
      "         2.9297e-02,  3.1250e-02,  3.5156e-02,  3.9062e-02,  4.2969e-02,\n",
      "         4.6875e-02,  5.0781e-02,  5.4688e-02,  5.8594e-02,  6.2500e-02,\n",
      "         7.0312e-02,  7.8125e-02,  8.5938e-02,  9.3750e-02,  1.0156e-01,\n",
      "         1.0938e-01,  1.1719e-01,  1.2500e-01,  1.4062e-01,  1.5625e-01,\n",
      "         1.7188e-01,  1.8750e-01,  2.0312e-01,  2.1875e-01,  2.3438e-01,\n",
      "         2.5000e-01,  2.8125e-01,  3.1250e-01,  3.4375e-01,  3.7500e-01,\n",
      "         4.0625e-01,  4.3750e-01,  4.6875e-01,  5.0000e-01,  5.6250e-01,\n",
      "         6.2500e-01,  6.8750e-01,  7.5000e-01,  8.1250e-01,  8.7500e-01,\n",
      "         9.3750e-01,  1.0000e+00,  1.1250e+00,  1.2500e+00,  1.3750e+00,\n",
      "         1.5000e+00,  1.6250e+00,  1.7500e+00,  1.8750e+00,  2.0000e+00,\n",
      "         2.2500e+00,  2.5000e+00,  2.7500e+00,  3.0000e+00,  3.2500e+00,\n",
      "         3.5000e+00,  3.7500e+00,  4.0000e+00,  4.5000e+00,  5.0000e+00,\n",
      "         5.5000e+00,  6.0000e+00,  6.5000e+00,  7.0000e+00,  7.5000e+00,\n",
      "         8.0000e+00,  9.0000e+00,  1.0000e+01,  1.1000e+01,  1.2000e+01,\n",
      "         1.3000e+01,  1.4000e+01,  1.5000e+01,  1.6000e+01,  1.8000e+01,\n",
      "         2.0000e+01,  2.2000e+01,  2.4000e+01,  2.6000e+01,  2.8000e+01,\n",
      "         3.0000e+01,  3.2000e+01,  3.6000e+01,  4.0000e+01,  4.4000e+01,\n",
      "         4.8000e+01,  5.2000e+01,  5.6000e+01,  6.0000e+01,  6.4000e+01,\n",
      "         7.2000e+01,  8.0000e+01,  8.8000e+01,  9.6000e+01,  1.0400e+02,\n",
      "         1.1200e+02,  1.2000e+02,  1.2800e+02,  1.4400e+02,  1.6000e+02,\n",
      "         1.7600e+02,  1.9200e+02,  2.0800e+02,  2.2400e+02,  2.4000e+02,\n",
      "                inf,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan, -0.0000e+00, -1.9531e-03,\n",
      "        -3.9062e-03, -5.8594e-03, -7.8125e-03, -9.7656e-03, -1.1719e-02,\n",
      "        -1.3672e-02, -1.5625e-02, -1.7578e-02, -1.9531e-02, -2.1484e-02,\n",
      "        -2.3438e-02, -2.5391e-02, -2.7344e-02, -2.9297e-02, -3.1250e-02,\n",
      "        -3.5156e-02, -3.9062e-02, -4.2969e-02, -4.6875e-02, -5.0781e-02,\n",
      "        -5.4688e-02, -5.8594e-02, -6.2500e-02, -7.0312e-02, -7.8125e-02,\n",
      "        -8.5938e-02, -9.3750e-02, -1.0156e-01, -1.0938e-01, -1.1719e-01,\n",
      "        -1.2500e-01, -1.4062e-01, -1.5625e-01, -1.7188e-01, -1.8750e-01,\n",
      "        -2.0312e-01, -2.1875e-01, -2.3438e-01, -2.5000e-01, -2.8125e-01,\n",
      "        -3.1250e-01, -3.4375e-01, -3.7500e-01, -4.0625e-01, -4.3750e-01,\n",
      "        -4.6875e-01, -5.0000e-01, -5.6250e-01, -6.2500e-01, -6.8750e-01,\n",
      "        -7.5000e-01, -8.1250e-01, -8.7500e-01, -9.3750e-01, -1.0000e+00,\n",
      "        -1.1250e+00, -1.2500e+00, -1.3750e+00, -1.5000e+00, -1.6250e+00,\n",
      "        -1.7500e+00, -1.8750e+00, -2.0000e+00, -2.2500e+00, -2.5000e+00,\n",
      "        -2.7500e+00, -3.0000e+00, -3.2500e+00, -3.5000e+00, -3.7500e+00,\n",
      "        -4.0000e+00, -4.5000e+00, -5.0000e+00, -5.5000e+00, -6.0000e+00,\n",
      "        -6.5000e+00, -7.0000e+00, -7.5000e+00, -8.0000e+00, -9.0000e+00,\n",
      "        -1.0000e+01, -1.1000e+01, -1.2000e+01, -1.3000e+01, -1.4000e+01,\n",
      "        -1.5000e+01, -1.6000e+01, -1.8000e+01, -2.0000e+01, -2.2000e+01,\n",
      "        -2.4000e+01, -2.6000e+01, -2.8000e+01, -3.0000e+01, -3.2000e+01,\n",
      "        -3.6000e+01, -4.0000e+01, -4.4000e+01, -4.8000e+01, -5.2000e+01,\n",
      "        -5.6000e+01, -6.0000e+01, -6.4000e+01, -7.2000e+01, -8.0000e+01,\n",
      "        -8.8000e+01, -9.6000e+01, -1.0400e+02, -1.1200e+02, -1.2000e+02,\n",
      "        -1.2800e+02, -1.4400e+02, -1.6000e+02, -1.7600e+02, -1.9200e+02,\n",
      "        -2.0800e+02, -2.2400e+02, -2.4000e+02,        -inf,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan])\n"
     ]
    }
   ],
   "source": [
    "# Construct the full range of uint8 for e4m3\n",
    "x_e4m3 = torch.arange(0, 2**8, dtype=torch.int32)\n",
    "s_e4m3, e_e4m3, m_e4m3 = decompose_8bit_e4m3(x_e4m3)\n",
    "\n",
    "# Generate fp8 e4m3 as floating points\n",
    "fp8_e4m3 = torch.zeros(256, dtype=torch.float32)\n",
    "\n",
    "for i in range(256):\n",
    "    sign = (i & 0b10000000) >> 7\n",
    "    exponent = (i & 0b01111000) >> 3\n",
    "    mantissa = i & 0b00000111\n",
    "\n",
    "    # Handle special cases\n",
    "    if exponent == 0:\n",
    "        if mantissa == 0:\n",
    "            # Zero\n",
    "            fp8_e4m3[i] = 0.0 if sign == 0 else -0.0\n",
    "        else:\n",
    "            # Subnormal numbers\n",
    "            fp8_e4m3[i] = ((-1)**sign) * (2**-6) * (mantissa / 8)\n",
    "    elif exponent == 15:\n",
    "        if mantissa == 0:\n",
    "            # Infinity\n",
    "            fp8_e4m3[i] = float('inf') if sign == 0 else float('-inf')\n",
    "        else:\n",
    "            # NaN\n",
    "            fp8_e4m3[i] = float('nan')\n",
    "    else:\n",
    "        # Normal numbers\n",
    "        fp8_e4m3[i] = ((-1)**sign) * (2**(exponent - 7)) * (1 + mantissa / 8)\n",
    "\n",
    "print(\"FP8 E4M3 range:\")\n",
    "print(fp8_e4m3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP8 E5M2 range:\n",
      "tensor([ 0.0000e+00,  1.5259e-05,  3.0518e-05,  4.5776e-05,  6.1035e-05,\n",
      "         7.6294e-05,  9.1553e-05,  1.0681e-04,  1.2207e-04,  1.5259e-04,\n",
      "         1.8311e-04,  2.1362e-04,  2.4414e-04,  3.0518e-04,  3.6621e-04,\n",
      "         4.2725e-04,  4.8828e-04,  6.1035e-04,  7.3242e-04,  8.5449e-04,\n",
      "         9.7656e-04,  1.2207e-03,  1.4648e-03,  1.7090e-03,  1.9531e-03,\n",
      "         2.4414e-03,  2.9297e-03,  3.4180e-03,  3.9062e-03,  4.8828e-03,\n",
      "         5.8594e-03,  6.8359e-03,  7.8125e-03,  9.7656e-03,  1.1719e-02,\n",
      "         1.3672e-02,  1.5625e-02,  1.9531e-02,  2.3438e-02,  2.7344e-02,\n",
      "         3.1250e-02,  3.9062e-02,  4.6875e-02,  5.4688e-02,  6.2500e-02,\n",
      "         7.8125e-02,  9.3750e-02,  1.0938e-01,  1.2500e-01,  1.5625e-01,\n",
      "         1.8750e-01,  2.1875e-01,  2.5000e-01,  3.1250e-01,  3.7500e-01,\n",
      "         4.3750e-01,  5.0000e-01,  6.2500e-01,  7.5000e-01,  8.7500e-01,\n",
      "         1.0000e+00,  1.2500e+00,  1.5000e+00,  1.7500e+00,  2.0000e+00,\n",
      "         2.5000e+00,  3.0000e+00,  3.5000e+00,  4.0000e+00,  5.0000e+00,\n",
      "         6.0000e+00,  7.0000e+00,  8.0000e+00,  1.0000e+01,  1.2000e+01,\n",
      "         1.4000e+01,  1.6000e+01,  2.0000e+01,  2.4000e+01,  2.8000e+01,\n",
      "         3.2000e+01,  4.0000e+01,  4.8000e+01,  5.6000e+01,  6.4000e+01,\n",
      "         8.0000e+01,  9.6000e+01,  1.1200e+02,  1.2800e+02,  1.6000e+02,\n",
      "         1.9200e+02,  2.2400e+02,  2.5600e+02,  3.2000e+02,  3.8400e+02,\n",
      "         4.4800e+02,  5.1200e+02,  6.4000e+02,  7.6800e+02,  8.9600e+02,\n",
      "         1.0240e+03,  1.2800e+03,  1.5360e+03,  1.7920e+03,  2.0480e+03,\n",
      "         2.5600e+03,  3.0720e+03,  3.5840e+03,  4.0960e+03,  5.1200e+03,\n",
      "         6.1440e+03,  7.1680e+03,  8.1920e+03,  1.0240e+04,  1.2288e+04,\n",
      "         1.4336e+04,  1.6384e+04,  2.0480e+04,  2.4576e+04,  2.8672e+04,\n",
      "         3.2768e+04,  4.0960e+04,  4.9152e+04,  5.7344e+04,         inf,\n",
      "                nan,         nan,         nan, -0.0000e+00, -1.5259e-05,\n",
      "        -3.0518e-05, -4.5776e-05, -6.1035e-05, -7.6294e-05, -9.1553e-05,\n",
      "        -1.0681e-04, -1.2207e-04, -1.5259e-04, -1.8311e-04, -2.1362e-04,\n",
      "        -2.4414e-04, -3.0518e-04, -3.6621e-04, -4.2725e-04, -4.8828e-04,\n",
      "        -6.1035e-04, -7.3242e-04, -8.5449e-04, -9.7656e-04, -1.2207e-03,\n",
      "        -1.4648e-03, -1.7090e-03, -1.9531e-03, -2.4414e-03, -2.9297e-03,\n",
      "        -3.4180e-03, -3.9062e-03, -4.8828e-03, -5.8594e-03, -6.8359e-03,\n",
      "        -7.8125e-03, -9.7656e-03, -1.1719e-02, -1.3672e-02, -1.5625e-02,\n",
      "        -1.9531e-02, -2.3438e-02, -2.7344e-02, -3.1250e-02, -3.9062e-02,\n",
      "        -4.6875e-02, -5.4688e-02, -6.2500e-02, -7.8125e-02, -9.3750e-02,\n",
      "        -1.0938e-01, -1.2500e-01, -1.5625e-01, -1.8750e-01, -2.1875e-01,\n",
      "        -2.5000e-01, -3.1250e-01, -3.7500e-01, -4.3750e-01, -5.0000e-01,\n",
      "        -6.2500e-01, -7.5000e-01, -8.7500e-01, -1.0000e+00, -1.2500e+00,\n",
      "        -1.5000e+00, -1.7500e+00, -2.0000e+00, -2.5000e+00, -3.0000e+00,\n",
      "        -3.5000e+00, -4.0000e+00, -5.0000e+00, -6.0000e+00, -7.0000e+00,\n",
      "        -8.0000e+00, -1.0000e+01, -1.2000e+01, -1.4000e+01, -1.6000e+01,\n",
      "        -2.0000e+01, -2.4000e+01, -2.8000e+01, -3.2000e+01, -4.0000e+01,\n",
      "        -4.8000e+01, -5.6000e+01, -6.4000e+01, -8.0000e+01, -9.6000e+01,\n",
      "        -1.1200e+02, -1.2800e+02, -1.6000e+02, -1.9200e+02, -2.2400e+02,\n",
      "        -2.5600e+02, -3.2000e+02, -3.8400e+02, -4.4800e+02, -5.1200e+02,\n",
      "        -6.4000e+02, -7.6800e+02, -8.9600e+02, -1.0240e+03, -1.2800e+03,\n",
      "        -1.5360e+03, -1.7920e+03, -2.0480e+03, -2.5600e+03, -3.0720e+03,\n",
      "        -3.5840e+03, -4.0960e+03, -5.1200e+03, -6.1440e+03, -7.1680e+03,\n",
      "        -8.1920e+03, -1.0240e+04, -1.2288e+04, -1.4336e+04, -1.6384e+04,\n",
      "        -2.0480e+04, -2.4576e+04, -2.8672e+04, -3.2768e+04, -4.0960e+04,\n",
      "        -4.9152e+04, -5.7344e+04,        -inf,         nan,         nan,\n",
      "                nan])\n"
     ]
    }
   ],
   "source": [
    "# Construct the full range of uint8\n",
    "x = torch.arange(0, 2**8, dtype=torch.int32)\n",
    "s, e, m = decompose_8bit_e5m2(x)\n",
    "# Generate fp8 e5m2 as floating points\n",
    "fp8_e5m2 = torch.zeros(256, dtype=torch.float32)\n",
    "\n",
    "for i in range(256):\n",
    "    sign = (i & 0b10000000) >> 7\n",
    "    exponent = (i & 0b01111100) >> 2\n",
    "    mantissa = i & 0b00000011\n",
    "\n",
    "    # Handle special cases\n",
    "    if exponent == 0:\n",
    "        if mantissa == 0:\n",
    "            # Zero\n",
    "            fp8_e5m2[i] = 0.0 if sign == 0 else -0.0\n",
    "        else:\n",
    "            # Subnormal numbers\n",
    "            fp8_e5m2[i] = ((-1)**sign) * (2**-14) * (mantissa / 4)\n",
    "    elif exponent == 31:\n",
    "        if mantissa == 0:\n",
    "            # Infinity\n",
    "            fp8_e5m2[i] = float('inf') if sign == 0 else float('-inf')\n",
    "        else:\n",
    "            # NaN\n",
    "            fp8_e5m2[i] = float('nan')\n",
    "    else:\n",
    "        # Normal numbers\n",
    "        fp8_e5m2[i] = ((-1)**sign) * (2**(exponent - 15)) * (1 + mantissa / 4)\n",
    "\n",
    "print(\"FP8 E5M2 range:\")\n",
    "print(fp8_e5m2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_inner_product_fp8(x, y, n_mantissa):\n",
    "    x_tensor = torch.tensor(x, dtype=torch.bfloat16)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.bfloat16)\n",
    "    \n",
    "    x_fp8 = round_to_fp8_represented_as_int8(x_tensor, n_mantissa, None)\n",
    "    y_fp8 = round_to_fp8_represented_as_int8(y_tensor, n_mantissa, None)\n",
    "    \n",
    "    x_fp8_float = undo_int8_fp8(x_fp8, n_mantissa)\n",
    "    y_fp8_float = undo_int8_fp8(y_fp8, n_mantissa)\n",
    "    \n",
    "    return torch.dot(x_fp8_float.flatten(), y_fp8_float.flatten()).item()\n",
    "\n",
    "def backward_error(true_result, computed_result):\n",
    "    return np.abs(true_result - computed_result) / np.abs(true_result)\n",
    "\n",
    "# Generate n values\n",
    "n_values = np.logspace(2, 5, num=19, dtype=int)\n",
    "\n",
    "# Initialize lists to store results\n",
    "backward_errors_e5m2 = []\n",
    "backward_errors_e4m3 = []\n",
    "\n",
    "# Compute inner products and backward errors\n",
    "for n in tqdm(n_values, desc=\"Computing errors\"):\n",
    "    # Generate x and y vectors at bfloat16 precision\n",
    "    x = torch.rand(n, dtype=torch.bfloat16).to(torch.float32).numpy()\n",
    "    y = torch.rand(n, dtype=torch.bfloat16).to(torch.float32).numpy()\n",
    "\n",
    "    \n",
    "    # Compute true result in double precision\n",
    "    true_result = np.dot(x, y)\n",
    "    \n",
    "    # Compute result in e5m2 precision\n",
    "    e5m2_result = compute_inner_product_fp8(x, y, 2)\n",
    "    \n",
    "    # Compute result in e4m3 precision\n",
    "    e4m3_result = compute_inner_product_fp8(x, y, 3)\n",
    "    \n",
    "    # Compute backward errors\n",
    "    error_e5m2 = backward_error(true_result, e5m2_result)\n",
    "    error_e4m3 = backward_error(true_result, e4m3_result)\n",
    "    \n",
    "    backward_errors_e5m2.append(error_e5m2)\n",
    "    backward_errors_e4m3.append(error_e4m3)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(n_values, backward_errors_e5m2, 'bo-', label='E5M2')\n",
    "plt.loglog(n_values, backward_errors_e4m3, 'ro-', label='E4M3')\n",
    "\n",
    "# Add sqrt(nu) line\n",
    "eps = 0.0625  # e5m2 epsilon (2^-4)\n",
    "sqrt_nu = np.sqrt(n_values) * eps\n",
    "plt.loglog(n_values, sqrt_nu, 'g--', label='sqrt(nu)')\n",
    "\n",
    "plt.xlabel('Vector length (n)')\n",
    "plt.ylabel('Backward error')\n",
    "plt.title('Backward Error of Inner Product in FP8 Precision')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"E5M2 results:\")\n",
    "print(f\"Minimum error: {min(backward_errors_e5m2):.2e}\")\n",
    "print(f\"Maximum error: {max(backward_errors_e5m2):.2e}\")\n",
    "print(f\"Mean error: {np.mean(backward_errors_e5m2):.2e}\")\n",
    "\n",
    "print(\"\\nE4M3 results:\")\n",
    "print(f\"Minimum error: {min(backward_errors_e4m3):.2e}\")\n",
    "print(f\"Maximum error: {max(backward_errors_e4m3):.2e}\")\n",
    "print(f\"Mean error: {np.mean(backward_errors_e4m3):.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are able to:\n",
    "- Construct the full range of bfloat16, uint8\n",
    "- Convert bfloat16 to bits and back\n",
    "- Convert bfloat16 to sign, exponent, mantissa and back\n",
    "- Test that conversion is correct except NaN values\n",
    "- Truncate when bfloat16 to e5m2 with mantissa chopping\n",
    "- Round up when converting from bfloat16 to e5m2\n",
    "- Round by stochastic rounding scheme\n",
    "\n",
    "Next:\n",
    "- Test using subnormal bfloat16 inputs\n",
    "- Test clamping to e5m2 and e4m3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
